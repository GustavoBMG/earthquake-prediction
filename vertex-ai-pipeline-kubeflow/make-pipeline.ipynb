{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install kfp\n",
    "# !pip install google-cloud-aiplatform\n",
    "# !pip install google-cloud-pipeline-components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.google.client import AIPlatformClient\n",
    "from google.cloud import aiplatform\n",
    "# from google.cloud.aiplatform import pipeline_jobs\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "\n",
    "from kfp import components as comp\n",
    "import kfp.dsl as dsl\n",
    "\n",
    "project_id = 'project-id'\n",
    "region = 'us-central1'\n",
    "pipeline_name = 'earthquake-prediction'\n",
    "\n",
    "main_bucket_name = 'bucket-name'\n",
    "pipeline_root_path = 'gs://' + main_bucket_name + '/pipelines/'\n",
    "\n",
    "model_name = '5390f2dc404cbe0cd01427a938160354'\n",
    "problem_statement_file = 'problem_statement/weekly.json'\n",
    "raw_location = 'data/quakes/raw/'\n",
    "silver_location = 'data/quakes/silver/'\n",
    "gold_location = 'data/quakes/gold/' + model_name + '/'\n",
    "artifact_location = 'models/' + model_name + '/'\n",
    "predictions_location = 'data/predictions/' + model_name + '/'\n",
    "metrics_location = 'data/metrics/' + model_name + '/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google\n",
    "\n",
    "def save_pipeline(pipeline, bucket_name, files_path):\n",
    "    \n",
    "    #### Get the bucket that the file will be uploaded to\n",
    "    storage_client = google.cloud.storage.Client()\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "\n",
    "    #### Create a new blob\n",
    "    my_file = bucket.blob(files_path + pipeline)\n",
    "\n",
    "    #### Upload from file\n",
    "    my_file.upload_from_filename(pipeline, content_type = 'application/json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_url(\n",
    "    url: str,\n",
    "    delta_days: int,\n",
    "    downloaded_data_path: comp.OutputPath('csv')\n",
    ") -> str:\n",
    "\n",
    "    import requests\n",
    "    from datetime import datetime, timedelta\n",
    "\n",
    "    # get time\n",
    "    request_time = datetime.now().astimezone().strftime('%Y-%m-%dT%H-%M-%S-%Z') + '/all_quakes.csv'\n",
    "\n",
    "    # get resquest params\n",
    "    end_range = datetime.now() + timedelta(days = 1)\n",
    "    begin_range = end_range - timedelta(days = delta_days)\n",
    "    end_range = end_range.strftime('%Y-%m-%d')\n",
    "    begin_range = begin_range.strftime('%Y-%m-%d')\n",
    "\n",
    "    query = {'format': 'csv', 'starttime': begin_range, 'endtime': end_range}\n",
    "\n",
    "    response_content = None\n",
    "\n",
    "    # make request\n",
    "    try:\n",
    "        with requests.get(url, params = query) as response:\n",
    "            response.raise_for_status()\n",
    "            response_content = response.content\n",
    "    except requests.exceptions.Timeout:\n",
    "        print('Timeout Exception')\n",
    "        return ''\n",
    "    except requests.exceptions.TooManyRedirects:\n",
    "        print('Too Many Redirects Exception')\n",
    "        return ''\n",
    "    except requests.exceptions.HTTPError:\n",
    "        print('Http Exception')\n",
    "        return ''\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print('Error Connecting')\n",
    "        return ''\n",
    "    except requests.exceptions.RequestException:\n",
    "        print('Request Exception')\n",
    "        return ''\n",
    "    except:\n",
    "        print('Error requesting file')\n",
    "        return ''\n",
    "\n",
    "    # dump for next component\n",
    "    with open(downloaded_data_path, 'w') as text_file:\n",
    "        text_file.write(response_content.decode('utf-8'))\n",
    "\n",
    "    return request_time\n",
    "\n",
    "get_data_from_url_op = comp.create_component_from_func(\n",
    "    get_data_from_url,\n",
    "    base_image = 'python:3.7',\n",
    "    packages_to_install = [\n",
    "        'requests',\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_to_gcp(\n",
    "    file_to_save_path: comp.InputPath('csv'),\n",
    "    file_name: str,\n",
    "    bucket_name: str,\n",
    "    bucket_folder: str,\n",
    "):\n",
    "\n",
    "    from google.cloud import storage\n",
    "    \n",
    "    # read from last step\n",
    "    with open(file_to_save_path, 'r') as text_file:\n",
    "        input_file = text_file.read()\n",
    "\n",
    "    # if None exit\n",
    "    if input_file is None:\n",
    "        print('No response')\n",
    "        return\n",
    "\n",
    "    # get the bucket that the file will be uploaded to\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    " \n",
    "    # create a new blob\n",
    "    my_file = bucket.blob(bucket_folder + file_name)\n",
    " \n",
    "    # upload from csv\n",
    "    my_file.upload_from_string(input_file, content_type = 'text/csv')\n",
    "\n",
    "save_data_to_gcp_op = comp.create_component_from_func(\n",
    "    save_data_to_gcp,\n",
    "    base_image = 'path-to-artifact-registry/python-gcp:basic',\n",
    "    packages_to_install = [],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(\n",
    "    csv_file_path: comp.InputPath('csv'),\n",
    "    cleaned_data_path: comp.OutputPath('csv'),\n",
    "):\n",
    "\n",
    "    import joblib\n",
    "    import pandas as pd\n",
    "    \n",
    "    # convert to DataFrame\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    \n",
    "    # select info to be used\n",
    "    df = df.loc[:, ['time', 'id', 'latitude', 'longitude', 'depth', 'mag']].copy()\n",
    "    \n",
    "    # weird values\n",
    "    z_0 = df['depth'] < 0\n",
    "    print(f'Depth above sup: {sum(z_0):,d} ({sum(z_0) / len(df):.2%})')\n",
    "    df.loc[z_0, 'depth'] = 0\n",
    "    \n",
    "    # data parsing\n",
    "    date_col = 'time'\n",
    "    datetimes = df[date_col].str.split('T', expand = True)\n",
    "    dates = pd.to_datetime(datetimes.loc[:, 0], format = '%Y-%m-%d')\n",
    "    df = pd.concat((df, dates.rename('date')), axis = 1)\n",
    "    df = df.drop(date_col, axis = 1)\n",
    "    \n",
    "    # drop NA\n",
    "    len_before = len(df)\n",
    "    df = df.dropna()\n",
    "    len_after = len(df)\n",
    "    dropped_events = len_before - len_after\n",
    "    if (dropped_events) == 0:\n",
    "        print('No dropped events')\n",
    "    else:\n",
    "        print(f'Dropped events: {dropped_events:,d} ({dropped_events / len_before:.2%})')\n",
    "    \n",
    "    # dump df for next component\n",
    "    joblib.dump(df, cleaned_data_path)\n",
    "\n",
    "clean_data_op = comp.create_component_from_func(\n",
    "    clean_data,\n",
    "    base_image = 'path-to-artifact-registry/python-pandas:basic',\n",
    "    packages_to_install = [],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_new_data(\n",
    "    new_data_path: comp.InputPath('csv'),\n",
    "    bucket_name: str,\n",
    "    file_path: str,\n",
    "    file_name: str,\n",
    "    silver_data_path: comp.OutputPath('csv'),\n",
    ") -> bool:\n",
    "\n",
    "    import joblib\n",
    "    import pandas as pd\n",
    "    \n",
    "    # read from last step\n",
    "    df_new = joblib.load(new_data_path)\n",
    "\n",
    "    if len(df_new) == 0:\n",
    "        return False\n",
    "\n",
    "    # read source file\n",
    "    df_hist = pd.read_csv('gs://' + bucket_name + '/' + file_path + file_name)\n",
    "    print(f'Silver events: {len(df_hist):,d}')\n",
    "\n",
    "    # check if there is new information\n",
    "    # TODO: check if there is change, not only new records\n",
    "    new_events = df_new[~df_new['id'].isin(df_hist['id'])].copy()\n",
    "    print(f'New events: {len(new_events):,d}')\n",
    "    \n",
    "    if (len(new_events) > 0):\n",
    "        \n",
    "        df = pd.concat([df_hist, new_events])\n",
    "        len_df, nunique_ids = len(df), df['id'].nunique()\n",
    "        \n",
    "        print(f'Total events: {len_df:,d} | Unique: {nunique_ids:,d}')\n",
    "        assert(len_df == nunique_ids)\n",
    "        \n",
    "        # dump df for next component\n",
    "        joblib.dump(df, silver_data_path)\n",
    "\n",
    "        return True\n",
    "    else:\n",
    "        # dump df for next component\n",
    "        joblib.dump(df_hist, silver_data_path)\n",
    "        \n",
    "        return False\n",
    "\n",
    "merge_new_data_op = comp.create_component_from_func(\n",
    "    merge_new_data,\n",
    "    base_image = 'path-to-artifact-registry/python-pandas:basic',\n",
    "    packages_to_install = [],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_df_to_gcp(\n",
    "    file_to_save_path: comp.InputPath('csv'),\n",
    "    file_name: str,\n",
    "    suffix: str,\n",
    "    bucket_name: str,\n",
    "    bucket_folder: str,\n",
    ") -> str:\n",
    "\n",
    "    import joblib\n",
    "    \n",
    "    # read from last step\n",
    "    df = joblib.load(file_to_save_path)\n",
    "\n",
    "    # if None exit\n",
    "    if df is None:\n",
    "        print('No response')\n",
    "        return\n",
    "    \n",
    "    # save to GCS\n",
    "    file_and_suffix = file_name\n",
    "    file_and_suffix += '_' + suffix + '.csv' if suffix != '' else '.csv'\n",
    "    df.to_csv('gs://' + bucket_name + '/' + bucket_folder + file_and_suffix, index = False)\n",
    "    \n",
    "    return 'saved'\n",
    "\n",
    "save_df_to_gcp_op = comp.create_component_from_func(\n",
    "    save_df_to_gcp,\n",
    "    base_image = 'path-to-artifact-registry/python-pandas:basic',\n",
    "    packages_to_install = [],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "def get_problem_statement(\n",
    "    bucket_name: str,\n",
    "    source_file: str,\n",
    ") -> NamedTuple(\n",
    "    'OpOutputs',\n",
    "    [\n",
    "        ('main_id', str),\n",
    "        ('time_ref', str),\n",
    "        ('time_frequency', str),\n",
    "        ('target_raw', str),\n",
    "        ('early_warning_number', int),\n",
    "        ('range_warning_number', int),\n",
    "        ('pad_df', int),\n",
    "        ('event_reference', float),\n",
    "        ('degrees_latitude_grid', int),\n",
    "        ('km_depth_grid', int),\n",
    "        ('min_latitude', int),\n",
    "        ('max_latitude', int),\n",
    "        ('min_longitude', int),\n",
    "        ('max_longitude', int),\n",
    "        ('time_cut', str),\n",
    "    ]\n",
    "):\n",
    "\n",
    "    import json\n",
    "    from google.cloud import storage\n",
    "    from collections import namedtuple\n",
    "\n",
    "    # get bucket\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "\n",
    "    # get blob\n",
    "    my_file = bucket.get_blob(source_file)\n",
    "\n",
    "    # download from json\n",
    "    problem_statement_config = json.loads(my_file.download_as_text())\n",
    "    \n",
    "    config = namedtuple(\n",
    "        'OpOutputs',\n",
    "        [\n",
    "            'main_id',\n",
    "            'time_ref',\n",
    "            'time_frequency',\n",
    "            'target_raw',\n",
    "            'early_warning_number',\n",
    "            'range_warning_number',\n",
    "            'pad_df',\n",
    "            'event_reference',\n",
    "            'degrees_latitude_grid',\n",
    "            'km_depth_grid',\n",
    "            'min_latitude',\n",
    "            'max_latitude',\n",
    "            'min_longitude',\n",
    "            'max_longitude',\n",
    "            'time_cut',\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return config(\n",
    "        problem_statement_config['main_id'],\n",
    "        problem_statement_config['time_ref'],\n",
    "        problem_statement_config['time_frequency'],\n",
    "        problem_statement_config['target_raw'],\n",
    "        problem_statement_config['early_warning_number'],\n",
    "        problem_statement_config['range_warning_number'],\n",
    "        problem_statement_config['pad_df'],\n",
    "        problem_statement_config['event_reference'],\n",
    "        problem_statement_config['degrees_latitude_grid'],\n",
    "        problem_statement_config['km_depth_grid'],\n",
    "        problem_statement_config['min_latitude'],\n",
    "        problem_statement_config['max_latitude'],\n",
    "        problem_statement_config['min_longitude'],\n",
    "        problem_statement_config['max_longitude'],\n",
    "        problem_statement_config['time_cut'],\n",
    "    )\n",
    "\n",
    "get_problem_statement_op = comp.create_component_from_func(\n",
    "    get_problem_statement,\n",
    "    base_image = 'path-to-artifact-registry/python-gcp:basic',\n",
    "    packages_to_install = [],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_and_preprocess(\n",
    "    silver_path: comp.InputPath('csv'),\n",
    "    event_reference: float,\n",
    "    min_latitude: int,\n",
    "    max_latitude: int,\n",
    "    min_longitude: int,\n",
    "    max_longitude: int,\n",
    "    time_cut: str,\n",
    "    filtered_path: comp.OutputPath('csv'),\n",
    "):\n",
    "\n",
    "    import joblib\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from datetime import datetime\n",
    "    import zucaml.zucaml as ml\n",
    "    \n",
    "    # read from last step\n",
    "    df = joblib.load(silver_path)\n",
    "    df['date'] = pd.to_datetime(df['date'], format = '%Y-%m-%d', exact = True)\n",
    "\n",
    "    # events\n",
    "    df['event'] = ((df['mag'] >= event_reference) * 1).astype(np.uint8)\n",
    "\n",
    "    # filter regions\n",
    "    df['keep'] = (df['latitude'] >= min_latitude) & (df['latitude'] <= max_latitude) & (df['longitude'] >= min_longitude) & (df['longitude'] <= max_longitude)\n",
    "    df = df.loc[df['keep']].copy().reset_index()\n",
    "    df = df.drop(['keep', 'index'], axis = 1)\n",
    "    \n",
    "    # filter time\n",
    "    time_cut = datetime.strptime(time_cut, '%Y-%m-%d')\n",
    "    df = df[df['date'] > time_cut]\n",
    "    df = df.reset_index().drop(['index',], axis = 1)\n",
    "    \n",
    "    # energy\n",
    "    df['energy'] = 5.24\n",
    "    df['energy'] += 1.44 * df['mag']\n",
    "    df['energy'] = np.power(10, df['energy'])\n",
    "\n",
    "    # print info\n",
    "    number_events = df['event'].sum()\n",
    "    min_date = df['date'].min()\n",
    "    max_date = df['date'].max()\n",
    "    \n",
    "    print(f'Min date:\\t{min_date}')\n",
    "    print(f'Max date:\\t{max_date}')\n",
    "    print(f'Number of events:\\t{number_events:,d}')\n",
    "    ml.print_memory(df)\n",
    "\n",
    "    # dump df for next component\n",
    "    joblib.dump(df, filtered_path)\n",
    "\n",
    "filter_and_preprocess_op = comp.create_component_from_func(\n",
    "    filter_and_preprocess,\n",
    "    base_image = 'path-to-artifact-registry/zucaml:basic',\n",
    "    packages_to_install = [],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "def reindex_df(\n",
    "    filtered_path: comp.InputPath('csv'),\n",
    "    degrees_latitude_grid: int,\n",
    "    km_depth_grid: int,\n",
    "    pad_df: int,\n",
    "    main_id: str,\n",
    "    time_ref: str,\n",
    "    time_frequency: str,\n",
    "    date_offset: int,\n",
    "    is_training: str,\n",
    "    grid_path: comp.OutputPath('csv'),\n",
    ") -> NamedTuple(\n",
    "    'OpOutputs',\n",
    "    [\n",
    "        ('x_degre_km', float),\n",
    "        ('y_degre_km', float),\n",
    "        ('dx', int),\n",
    "        ('dy', int),\n",
    "        ('range_x_min', int),\n",
    "        ('range_x_max', int),\n",
    "        ('range_x_step', int),\n",
    "        ('range_y_min', int),\n",
    "        ('range_y_max', int),\n",
    "        ('range_y_step', int),\n",
    "        ('range_z_min', int),\n",
    "        ('range_z_max', int),\n",
    "        ('range_z_step', int),\n",
    "    ]\n",
    "):\n",
    "\n",
    "    import joblib\n",
    "    from collections import namedtuple\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from datetime import datetime, timedelta\n",
    "    import zucaml.zucaml as ml\n",
    "    \n",
    "    # read from last step\n",
    "    df = joblib.load(filtered_path)\n",
    "\n",
    "    # grid\n",
    "    x_degre_km = 94.2\n",
    "    y_degre_km = 111.2\n",
    "\n",
    "    dy = degrees_latitude_grid\n",
    "    dx = int(round(dy * y_degre_km / x_degre_km))\n",
    "    dz = km_depth_grid\n",
    "\n",
    "    grid_values = [\n",
    "        ('y', 'latitude', dy),\n",
    "        ('x', 'longitude', dx),\n",
    "        ('z', 'depth', dz)\n",
    "    ]\n",
    "\n",
    "    for new_feature, old_feature, increment in grid_values:\n",
    "\n",
    "        old_feature_min = int(round(df[old_feature].min()))\n",
    "\n",
    "        df[new_feature] = df[old_feature] - old_feature_min\n",
    "        df[new_feature] = df[new_feature] / increment\n",
    "        df[new_feature] = df[new_feature].round().astype(int)\n",
    "        df[new_feature] = df[new_feature] * increment\n",
    "        df[new_feature] = df[new_feature] + old_feature_min\n",
    "\n",
    "    assert(sum(df['z'] < 0) == 0)\n",
    "\n",
    "    df['zone_frame'] = df['x'].astype(str) + '|' + df['y'].astype(str) + '|' + df['z'].astype(str)\n",
    "\n",
    "    min_x = df['x'].min()\n",
    "    max_x = df['x'].max()\n",
    "    min_y = df['y'].min()\n",
    "    max_y = df['y'].max()\n",
    "    min_z = df['z'].min()\n",
    "    max_z = df['z'].max()\n",
    "\n",
    "    range_x = range(min_x, max_x + dx, dx)\n",
    "    range_y = range(min_y, max_y + dy, dy)\n",
    "    range_z = range(min_z, max_z + dz, dz)\n",
    "\n",
    "    all_zone_frames = [str(x) + '|' + str(y) + '|' + str(z) for x in range_x for y in range_y for z in range_z]\n",
    "\n",
    "    used_x = df['x'].nunique()\n",
    "    used_y = df['y'].nunique()\n",
    "    used_z = df['z'].nunique()\n",
    "    used_time = df['date'].nunique()\n",
    "\n",
    "    print(f'Unique x:\\t\\t{used_x:,d}')\n",
    "    print(f'Unique y:\\t\\t{used_y:,d}')\n",
    "    print(f'Unique z:\\t\\t{used_z:,d}')\n",
    "    print(f'Unique time:\\t\\t{used_time:,d}')\n",
    "    print(f'All zones:\\t\\t{len(all_zone_frames):,d}')\n",
    "\n",
    "    df = df.drop(['longitude', 'latitude', 'depth'], axis = 1)\n",
    "    \n",
    "    # offset date\n",
    "    min_date = df['date'].min()\n",
    "    print(f'Offset date. Min before: {str(min_date)} Records: {len(df):,d}')\n",
    "    \n",
    "    min_date = min_date + timedelta(days = date_offset)\n",
    "    df = df.loc[df['date'] >= min_date].copy().reset_index().drop('index', axis = 1)\n",
    "    \n",
    "    min_date = df['date'].min()\n",
    "    print(f'Offset date. Min after: {str(min_date)} Records: {len(df):,d}')\n",
    "    \n",
    "    # pad\n",
    "    if is_training == 'True':\n",
    "        pad_df = bool(pad_df)\n",
    "    else:\n",
    "        pad_df = True\n",
    "        this_max_date = df['date'].max()\n",
    "        ref_day = datetime.now()\n",
    "        print(f'date: {str(this_max_date)} - reference day: {str(ref_day)}')\n",
    "        if (this_max_date < ref_day):\n",
    "            print('Adding dummy')\n",
    "            len_before = len(df)\n",
    "            \n",
    "            record_dummy = df.iloc[0].copy()\n",
    "            record_dummy['date'] = ref_day\n",
    "            record_dummy['id'] = 'non_existant'\n",
    "            for feature in ['mag', 'event', 'energy']:\n",
    "                record_dummy[feature] = 0\n",
    "                \n",
    "            df = df.append(record_dummy, ignore_index = True)\n",
    "            \n",
    "            len_after = len(df)\n",
    "            print(f'Rows before: {len_before:,d} - Rows after: {len_after:,d}')\n",
    "            assert(len_after - len_before == 1)\n",
    "            new_max_date = df['date'].max()\n",
    "            print(f'New max date: {str(new_max_date)}')\n",
    "    if pad_df:\n",
    "\n",
    "        zero_fill = ['mag', 'event', 'energy']\n",
    "        other_fill = {'id': 'non_existant'}\n",
    "\n",
    "        df = ml.pad(df, 'zone_frame', 'date', all_zone_frames, 'min', zero_fill, other_fill)\n",
    "        df = ml.pad(df, 'zone_frame', 'date', all_zone_frames, 'max', zero_fill, other_fill)\n",
    "\n",
    "        df['x'] = df['zone_frame'].str.split('|').str[0].astype(int)\n",
    "        df['y'] = df['zone_frame'].str.split('|').str[1].astype(int)\n",
    "        df['z'] = df['zone_frame'].str.split('|').str[2].astype(int)\n",
    "\n",
    "        assert(df.isna().sum().sum() == 0)\n",
    "        \n",
    "    # reindex\n",
    "    df = ml.reindex_by_minmax(\n",
    "        df = df.drop(['mag', 'x', 'y', 'z'], axis = 1),\n",
    "        item = main_id,\n",
    "        time_ref = time_ref,\n",
    "        time_freq = time_frequency,\n",
    "        forwardfill_features = [],\n",
    "        backfill_features = [],\n",
    "        zerofill_features = ['energy', 'event'],\n",
    "    )\n",
    "\n",
    "    assert(df.isna().sum().sum() == 0)\n",
    "\n",
    "    df['event'] = ((df['event'] > 0) * 1).astype(np.uint8)\n",
    "\n",
    "    df['x'] = df['zone_frame'].str.split('|').str[0].astype(int)\n",
    "    df['y'] = df['zone_frame'].str.split('|').str[1].astype(int)\n",
    "    df['z'] = df['zone_frame'].str.split('|').str[2].astype(int)\n",
    "\n",
    "    # print info\n",
    "    ml.print_memory(df)\n",
    "    \n",
    "    # dump df for next component\n",
    "    joblib.dump(df, grid_path)\n",
    "    \n",
    "    # return variables\n",
    "    variables = namedtuple(\n",
    "        'OpOutputs',\n",
    "        [\n",
    "            'x_degre_km',\n",
    "            'y_degre_km',\n",
    "            'dx',\n",
    "            'dy',\n",
    "            'range_x_min',\n",
    "            'range_x_max',\n",
    "            'range_x_step',\n",
    "            'range_y_min',\n",
    "            'range_y_max',\n",
    "            'range_y_step',\n",
    "            'range_z_min',\n",
    "            'range_z_max',\n",
    "            'range_z_step',\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return variables(\n",
    "        x_degre_km,\n",
    "        y_degre_km,\n",
    "        dx,\n",
    "        dy,\n",
    "        range_x.start,\n",
    "        range_x.stop,\n",
    "        range_x.step,\n",
    "        range_y.start,\n",
    "        range_y.stop,\n",
    "        range_y.step,\n",
    "        range_z.start,\n",
    "        range_z.stop,\n",
    "        range_z.step,\n",
    "    )\n",
    "\n",
    "reindex_df_op = comp.create_component_from_func(\n",
    "    reindex_df,\n",
    "    base_image = 'path-to-artifact-registry/zucaml:basic',\n",
    "    packages_to_install = [],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighbours_df(\n",
    "    grid_path: comp.InputPath('csv'),\n",
    "    x_degre_km: float,\n",
    "    y_degre_km: float,\n",
    "    dx: int,\n",
    "    dy: int,\n",
    "    range_x_min: int,\n",
    "    range_x_max: int,\n",
    "    range_x_step: int,\n",
    "    range_y_min: int,\n",
    "    range_y_max: int,\n",
    "    range_y_step: int,\n",
    "    range_z_min: int,\n",
    "    range_z_max: int,\n",
    "    range_z_step: int,\n",
    "    full_path: comp.OutputPath('csv'),\n",
    "):\n",
    "\n",
    "    import joblib\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from datetime import datetime\n",
    "    import zucaml.zucaml as ml\n",
    "    \n",
    "    # read from last step\n",
    "    df = joblib.load(grid_path)\n",
    "\n",
    "    #### aux func\n",
    "    def get_xyz(zone_frame):\n",
    "\n",
    "        x, y, z = zone_frame.split('|')\n",
    "\n",
    "        x = int(x)\n",
    "        y = int(y)\n",
    "        z = int(z)\n",
    "\n",
    "        return x, y, z\n",
    "\n",
    "    #### get neighbours\n",
    "    def get_neighbours(zone_frame, neighbours, used_zone_frames):\n",
    "\n",
    "        this_neighbours = []\n",
    "\n",
    "        this_x, this_y, this_z = get_xyz(zone_frame)\n",
    "\n",
    "        for zf in used_zone_frames:\n",
    "            x, y, z = get_xyz(zf)\n",
    "\n",
    "            if zone_frame != zf and x in neighbours['x'][this_x] and y in neighbours['y'][this_y] and z in neighbours['z'][this_z]:\n",
    "                this_neighbours.append(zf)\n",
    "\n",
    "        return this_neighbours\n",
    "\n",
    "    #### calculate distance in xy plane\n",
    "    distance = max(x_degre_km * dx, y_degre_km * dy)\n",
    "\n",
    "    #### aux variable\n",
    "    range_x = range(range_x_min, range_x_max, range_x_step)\n",
    "    range_y = range(range_y_min, range_y_max, range_y_step)\n",
    "    range_z = range(range_z_min, range_z_max, range_z_step)\n",
    "    ranges = {'x': range_x, 'y': range_y}\n",
    "\n",
    "    #### neighbours coordinates\n",
    "    neighbours = {}\n",
    "\n",
    "    #### neighbours coordinates - xy\n",
    "    for dim in ['x', 'y']:\n",
    "\n",
    "        neighbours[dim] = {}\n",
    "\n",
    "        ordered = {}\n",
    "\n",
    "        for i, d in enumerate(ranges[dim]):\n",
    "            ordered[i] = d\n",
    "\n",
    "        for i, d in ordered.items():\n",
    "            neighbours[dim][d] = [d]\n",
    "            if i > 0:\n",
    "                neighbours[dim][d].append(ordered[i - 1])\n",
    "            if i < len(ordered) - 1:\n",
    "                neighbours[dim][d].append(ordered[i + 1])\n",
    "\n",
    "    #### neighbours coordinates - z\n",
    "    neighbours['z'] = {}\n",
    "\n",
    "    for z in range_z:\n",
    "\n",
    "        neighbours['z'][z] = [z]\n",
    "\n",
    "        for z2 in range_z:\n",
    "            if abs(z - z2) <= distance and z != z2:\n",
    "                 neighbours['z'][z].append(z2)\n",
    "\n",
    "    #### neighbours per zone frame\n",
    "    zone_frames_neighbours = {}\n",
    "\n",
    "    used_zone_frames = df['zone_frame'].unique()\n",
    "\n",
    "    for zone_frame in used_zone_frames:\n",
    "        zone_frames_neighbours[zone_frame] = get_neighbours(zone_frame, neighbours, used_zone_frames)\n",
    "\n",
    "    def get_energy_neighbours(df, used_zone_frames, zone_frames_neighbours):\n",
    "\n",
    "        dfs = []\n",
    "\n",
    "        for zone_frame in used_zone_frames:\n",
    "\n",
    "            df_zone = df.loc[df['zone_frame'] == zone_frame].copy()\n",
    "\n",
    "            df_zone_neighbours = df.loc[df['zone_frame'].isin(zone_frames_neighbours[zone_frame])].copy()\n",
    "\n",
    "            df_zone_neighbours = df_zone_neighbours.groupby(['date']).agg({'energy': np.sum}).reset_index()\n",
    "\n",
    "            new_feature = 'neighbours_' + zone_frame\n",
    "\n",
    "            df_zone_neighbours = df_zone_neighbours.rename({'energy': new_feature}, axis = 1)\n",
    "\n",
    "            df_zone_neighbours = df_zone_neighbours.loc[df_zone_neighbours[new_feature] != 0].copy()\n",
    "\n",
    "            df_zone = pd.merge(\n",
    "                df_zone,\n",
    "                df_zone_neighbours,\n",
    "                how = 'left',\n",
    "                on = ['date'],\n",
    "                suffixes = ['_repeated_left', 'repeated_right'],\n",
    "            )\n",
    "\n",
    "            dfs.append(df_zone)\n",
    "\n",
    "        dfs = pd.concat(dfs)\n",
    "\n",
    "        for feat in dfs:\n",
    "            if 'repeated' in feat:\n",
    "                print(f'Warning: repeated features')\n",
    "\n",
    "        assert(len(dfs) == len(df))\n",
    "\n",
    "        neighbours_features = [feat for feat in dfs if feat.startswith('neighbours_')]\n",
    "\n",
    "        dfs['energy_neighbours'] = dfs[neighbours_features].T.sum().T\n",
    "\n",
    "        dfs = dfs.drop(neighbours_features, axis = 1)\n",
    "\n",
    "        assert(dfs.isna().sum().sum() == 0)\n",
    "\n",
    "        return dfs\n",
    "\n",
    "    df = get_energy_neighbours(df, used_zone_frames, zone_frames_neighbours)\n",
    "\n",
    "    # print info\n",
    "    ml.print_memory(df)\n",
    "    \n",
    "    # dump df for next component\n",
    "    joblib.dump(df, full_path)\n",
    "\n",
    "get_neighbours_df_op = comp.create_component_from_func(\n",
    "    get_neighbours_df,\n",
    "    base_image = 'path-to-artifact-registry/zucaml:basic',\n",
    "    packages_to_install = [],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_problem_statement(\n",
    "    full_path: comp.InputPath('csv'),\n",
    "    main_id: str,\n",
    "    time_ref: str,\n",
    "    target_raw: str,\n",
    "    early_warning_number: int,\n",
    "    range_warning_number: int,\n",
    "    is_training: str,\n",
    "    ps_path: comp.OutputPath('csv'),\n",
    "):\n",
    "\n",
    "    import joblib\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from datetime import datetime\n",
    "    import zucaml.zucaml as ml\n",
    "    \n",
    "    # read from last step\n",
    "    df = joblib.load(full_path)\n",
    "    \n",
    "    # set target\n",
    "    drop_na_target = is_training == 'True'\n",
    "    df = ml.set_target(\n",
    "        df = df,\n",
    "        item = main_id,\n",
    "        time_ref = time_ref,\n",
    "        target = target_raw,\n",
    "        early_warning = early_warning_number,\n",
    "        range_warning = range_warning_number,\n",
    "        drop_na_target = drop_na_target,\n",
    "    )\n",
    "\n",
    "    balance = df['target'].sum() / len(df)\n",
    "    print(f'Balance: {balance:.4%}')\n",
    "\n",
    "    # print info\n",
    "    ml.print_memory(df)\n",
    "\n",
    "    # dump df for next component\n",
    "    joblib.dump(df, ps_path)\n",
    "\n",
    "set_problem_statement_op = comp.create_component_from_func(\n",
    "    set_problem_statement,\n",
    "    base_image = 'path-to-artifact-registry/zucaml:basic',\n",
    "    packages_to_install = [],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(\n",
    "    ps_path: comp.InputPath('csv'),\n",
    "    main_id: str,\n",
    "    time_ref: str,\n",
    "    target_raw: str,\n",
    "    gold_data_path: comp.OutputPath('csv'),\n",
    "):\n",
    "\n",
    "    import joblib\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from datetime import datetime\n",
    "    import zucaml.zucaml as ml\n",
    "    \n",
    "    # read from last step\n",
    "    df = joblib.load(ps_path)\n",
    "    \n",
    "    # create reset\n",
    "    df = ml.create_reset(\n",
    "        df = df,\n",
    "        item = main_id,\n",
    "        time_ref = time_ref,\n",
    "        order = None\n",
    "    )\n",
    "    \n",
    "    # M.A.\n",
    "    for window_rolling_mean in [30, 90, 180, 330, 360]:\n",
    "        df = ml.ts_feature(\n",
    "            df = df,\n",
    "            feature_base = 'energy',\n",
    "            func = 'rolling.mean',\n",
    "            func_val = window_rolling_mean,\n",
    "            label = None,\n",
    "        )\n",
    "\n",
    "    for window_rolling_mean in [30, 90, 180, 330, 360]:\n",
    "        df = ml.ts_feature(\n",
    "            df = df,\n",
    "            feature_base = 'energy_neighbours',\n",
    "            func = 'rolling.mean',\n",
    "            func_val = window_rolling_mean,\n",
    "            label = None,\n",
    "        )\n",
    "        \n",
    "    # ratios\n",
    "    df = ml.math_feature(\n",
    "        df = df,\n",
    "        feature_1 = 'energy|rolling.mean#30',\n",
    "        feature_2 = 'energy|rolling.mean#360',\n",
    "        func = 'ratio',\n",
    "        label = None,\n",
    "    )\n",
    "\n",
    "    df = ml.math_feature(\n",
    "        df = df,\n",
    "        feature_1 = 'energy|rolling.mean#90',\n",
    "        feature_2 = 'energy|rolling.mean#360',\n",
    "        func = 'ratio',\n",
    "        label = None,\n",
    "    )\n",
    "\n",
    "    df = ml.math_feature(\n",
    "        df = df,\n",
    "        feature_1 = 'energy|rolling.mean#180',\n",
    "        feature_2 = 'energy|rolling.mean#360',\n",
    "        func = 'ratio',\n",
    "        label = None,\n",
    "    )\n",
    "\n",
    "    df = ml.math_feature(\n",
    "        df = df,\n",
    "        feature_1 = 'energy|rolling.mean#330',\n",
    "        feature_2 = 'energy|rolling.mean#360',\n",
    "        func = 'ratio',\n",
    "        label = None,\n",
    "    )\n",
    "    \n",
    "    # track last event\n",
    "    df = ml.track_feature(\n",
    "        df = df,\n",
    "        feature_base = time_ref,\n",
    "        condition = df[target_raw] > 0,\n",
    "        track_window = 0,\n",
    "        track_function = 'diff.days',\n",
    "        label = 'days.since.last'\n",
    "    )\n",
    "\n",
    "    # clean and order\n",
    "    df = df.drop('reset', axis = 1)\n",
    "    df = df.sort_values(['zone_frame', 'date']).reset_index().drop('index', axis = 1)\n",
    "\n",
    "    # print info\n",
    "    balance = df['target'].sum() / len(df)\n",
    "    print(f'{balance:.6%}')\n",
    "    ml.print_memory(df)\n",
    "\n",
    "    # dump df for next component\n",
    "    joblib.dump(df, gold_data_path)\n",
    "\n",
    "feature_engineering_op = comp.create_component_from_func(\n",
    "    feature_engineering,\n",
    "    base_image = 'path-to-artifact-registry/zucaml:basic',\n",
    "    packages_to_install = [],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(\n",
    "    gold_data_path: comp.InputPath('csv'),\n",
    "    bucket_name: str,\n",
    "    artifact_folder: str,\n",
    "    step_x: int,\n",
    "    step_y: int,\n",
    "    step_z: int,\n",
    "    time_frequency: str,\n",
    "    early_warning_number: int,\n",
    "    range_warning_number: int,\n",
    "    date_offset: int,\n",
    "    predictions_data_path: comp.OutputPath('csv'),\n",
    ") -> str:\n",
    "\n",
    "    import joblib\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import json\n",
    "    from datetime import datetime, timedelta\n",
    "    from pickle import loads\n",
    "    from google.cloud import storage\n",
    "    import zucaml.zucaml as ml\n",
    "    \n",
    "    # read from last step\n",
    "    df = joblib.load(gold_data_path)\n",
    "\n",
    "    # get bucket\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "\n",
    "    # get blobs\n",
    "    config_file = bucket.get_blob(artifact_folder + 'notes.txt')\n",
    "    model_file = bucket.get_blob(artifact_folder + 'model.pkl')\n",
    "\n",
    "    # download\n",
    "    model_config = json.loads(config_file.download_as_text())\n",
    "    model = loads(model_file.download_as_string())\n",
    "    \n",
    "    # select max date\n",
    "    df = df.loc[df['date'] == df['date'].max()].copy().reset_index().drop('index', axis = 1)\n",
    "    \n",
    "    # make predictions\n",
    "    df['probability'] = model.predict_proba(df[model_config['features']])[:, 1]\n",
    "    df['prediction'] = (df['probability'] > model_config['threshold']) * 1\n",
    "    \n",
    "    # select fields\n",
    "    df = df.loc[:, ['x', 'y', 'z', 'date', 'probability', 'prediction']].copy()\n",
    "    \n",
    "    # transform fields\n",
    "    for location, step in [('x', step_x), ('y', step_y), ('z', step_z)]:\n",
    "        df[location + '_min'] = df[location] - int(step / 2)\n",
    "        df[location + '_max'] = df[location] + int(step / 2)\n",
    "    df.loc[df['z_min'] < 0, 'z_min'] = 0\n",
    "    if 'D' in time_frequency:\n",
    "        time_frequency_value = int(time_frequency.replace('D', ''))\n",
    "        df['date_min'] = df['date'] + timedelta(days = early_warning_number * time_frequency_value)\n",
    "        df['date_max'] = df['date_min'] + timedelta(days = range_warning_number * time_frequency_value)\n",
    "    elif 'W' in time_frequency:\n",
    "        time_frequency_value = int(time_frequency.replace('W', ''))\n",
    "        df['date_min'] = df['date'] + timedelta(weeks = early_warning_number * time_frequency_value)\n",
    "        df['date_max'] = df['date_min'] + timedelta(weeks = range_warning_number * time_frequency_value)\n",
    "    else:\n",
    "        print(f'Unknown timefrequency: {str(time_frequency)}')\n",
    "    df['timestamp'] = datetime.now()\n",
    "    df['offset'] = date_offset\n",
    "    \n",
    "    # check if is to predict\n",
    "    dataframe_date = df['date'].iloc[0]\n",
    "    this_day = datetime.now().date()\n",
    "    print(f'date: {str(dataframe_date)} - today: {str(this_day)}')\n",
    "    if dataframe_date >= this_day:\n",
    "        \n",
    "        df = df.drop(['x', 'y', 'z', 'date'], axis = 1)\n",
    "        \n",
    "        # print info\n",
    "        number_predictions = df['prediction'].sum()\n",
    "        print(f'Prediction true: {number_predictions:,d}')\n",
    "        ml.print_memory(df)\n",
    "\n",
    "        # dump df for next component\n",
    "        joblib.dump(df, predictions_data_path)\n",
    "    \n",
    "        return str(date_offset) + '_' + datetime.now().astimezone().strftime('%Y-%m-%dT%H-%M-%S-%Z')\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "make_predictions_op = comp.create_component_from_func(\n",
    "    make_predictions,\n",
    "    base_image = 'path-to-artifact-registry/zucaml:basic',\n",
    "    packages_to_install = ['google-cloud-storage', 'gcsfs', 'fsspec'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(\n",
    "    dummy_input: str,\n",
    "    main_bucket_name: str,\n",
    "    silver_location: str,\n",
    "    predictions_location: str,\n",
    "    metrics_location: str,\n",
    "    event_reference: float,\n",
    "    min_latitude: int,\n",
    "    max_latitude: int,\n",
    "    min_longitude: int,\n",
    "    max_longitude: int,\n",
    "):\n",
    "\n",
    "    from os import listdir\n",
    "    from datetime import datetime, timedelta\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from google.cloud import storage\n",
    "    \n",
    "    # read predictions\n",
    "    storage_client = storage.Client()\n",
    "    onlyfiles = [f.name for f in storage_client.list_blobs(main_bucket_name, prefix = predictions_location) if f.name.endswith('.csv')]\n",
    "    dfs = []\n",
    "    for f in onlyfiles:\n",
    "        dfs.append(pd.read_csv('gs://' + main_bucket_name + '/' + f))\n",
    "    predictions = pd.concat(dfs)\n",
    "    predictions['date_min'] = pd.to_datetime(predictions['date_min'])\n",
    "    predictions['date_max'] = pd.to_datetime(predictions['date_max'])\n",
    "    first_prediction = predictions['date_min'].min()\n",
    "\n",
    "    # filter predictions\n",
    "    predictions = predictions.loc[predictions['prediction'] == 1].copy().reset_index().drop('index', axis = 1)\n",
    "\n",
    "    # id prediction\n",
    "    predictions = predictions.reset_index().rename({'index': 'id_pred'}, axis = 1)\n",
    "    total_predictions = predictions[predictions['date_min'] <= datetime.now()]['id_pred'].nunique()\n",
    "    \n",
    "    # read events\n",
    "    events = pd.read_csv('gs://' + main_bucket_name + '/' + silver_location + 'silver.csv')\n",
    "    events['date'] = pd.to_datetime(events['date'])\n",
    "    assert(events['id'].nunique() == len(events))\n",
    "\n",
    "    # filter events\n",
    "    study = events['mag'] >= event_reference\n",
    "    study = study & (events['longitude'] >= min_longitude)\n",
    "    study = study & (events['longitude'] <= max_longitude)\n",
    "    study = study & (events['latitude'] >= min_latitude)\n",
    "    study = study & (events['latitude'] <= max_latitude)\n",
    "    events = events.loc[(study) & (events['date'] >= first_prediction)].copy().reset_index().drop('index', axis = 1)\n",
    "\n",
    "    for dimension in ['longitude', 'latitude', 'depth']:\n",
    "        events[dimension] = events[dimension].round()\n",
    "    \n",
    "    # get tp, fp and fn\n",
    "    events_values = events['date'].values\n",
    "    predictions_min = predictions['date_min'].values\n",
    "    predictions_max = predictions['date_max'].values\n",
    "\n",
    "    i, j = np.where((events_values[:, None] >= predictions_min) & (events_values[:, None] <= predictions_max))\n",
    "\n",
    "    joined = pd.DataFrame(\n",
    "        np.column_stack([events.values[i], predictions.values[j]]),\n",
    "        columns = events.columns.append(predictions.columns)\n",
    "    )\n",
    "    \n",
    "    joined['keep'] = True\n",
    "    joined['keep'] = joined['keep'] & (joined['longitude'] >= joined['x_min'])\n",
    "    joined['keep'] = joined['keep'] & (joined['longitude'] <= joined['x_max'])\n",
    "    joined['keep'] = joined['keep'] & (joined['latitude'] >= joined['y_min'])\n",
    "    joined['keep'] = joined['keep'] & (joined['latitude'] <= joined['y_max'])\n",
    "    joined['keep'] = joined['keep'] & (joined['depth'] >= joined['z_min'])\n",
    "    joined['keep'] = joined['keep'] & (joined['depth'] <= joined['z_max'])\n",
    "    joined = joined.loc[joined['keep']].copy().reset_index().drop('index', axis = 1)\n",
    "    \n",
    "    events['predicted'] = 'Missed'\n",
    "    events.loc[events['id'].isin(joined['id']), 'predicted'] = 'Predicted'\n",
    "    \n",
    "    predictions['correct'] = 'False alarm'\n",
    "    predictions.loc[predictions['id_pred'].isin(joined['id_pred']), 'correct'] = 'Correct'\n",
    "    predictions.loc[(predictions['date_max'] > datetime.now() - timedelta(days = 1)) & (predictions['correct'] != 'Correct'), 'correct'] = ''\n",
    "    \n",
    "    # get metrics\n",
    "    number_earthquakes_predicted = sum(events['predicted'] == 'Predicted')\n",
    "    number_earthquakes_missed = sum(events['predicted'] == 'Missed')\n",
    "    number_predictions_correct = sum(predictions['correct'] == 'Correct')\n",
    "    number_predictions_false = sum(predictions['correct'] == 'False alarm')\n",
    "\n",
    "    epsilon = np.finfo(float).eps\n",
    "    precision = number_predictions_correct / (number_predictions_correct + number_predictions_false + epsilon)\n",
    "    recall = number_earthquakes_predicted / (number_earthquakes_predicted + number_earthquakes_missed + epsilon)\n",
    "\n",
    "    beta = 0.5\n",
    "    f05 = (1.0 + beta ** 2) * (precision * recall) / ((beta ** 2 * precision) + recall + epsilon)\n",
    "    beta = 1.0\n",
    "    f1 = (1.0 + beta ** 2) * (precision * recall) / ((beta ** 2 * precision) + recall + epsilon)\n",
    "\n",
    "    metrics = pd.DataFrame({\n",
    "        'Predicted': [number_earthquakes_predicted],\n",
    "        'Missed': [number_earthquakes_missed],\n",
    "        'Correct': [number_predictions_correct],\n",
    "        'False alarm': [number_predictions_false],\n",
    "        'Precision': [precision],\n",
    "        'Recall': [recall],\n",
    "        'F0.5': [f05],\n",
    "        'F1': [f1]\n",
    "    })\n",
    "\n",
    "    # dump files\n",
    "    events.to_csv('gs://' + main_bucket_name + '/' + metrics_location + 'events.csv', index = False)\n",
    "    predictions.to_csv('gs://' + main_bucket_name + '/' + metrics_location + 'predictions.csv', index = False)\n",
    "    metrics.to_csv('gs://' + main_bucket_name + '/' + metrics_location + 'metrics.csv', index = False)\n",
    "\n",
    "calculate_metrics_op = comp.create_component_from_func(\n",
    "    calculate_metrics,\n",
    "    base_image = 'path-to-artifact-registry/python-pandas:basic',\n",
    "    packages_to_install = ['google-cloud-storage'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(\n",
    "    name = pipeline_name,\n",
    "    pipeline_root = pipeline_root_path\n",
    ")\n",
    "\n",
    "def pipeline(\n",
    "    is_training: str,\n",
    "    maint_bucket: str,\n",
    "    problem_statement_file: str,\n",
    "    date_offset: int,\n",
    "    raw_location: str,\n",
    "    silver_location: str,\n",
    "    gold_location: str,\n",
    "    artifact_location: str,\n",
    "    predictions_location: str,\n",
    "    metrics_location: str,\n",
    "):\n",
    "\n",
    "    raw_data = get_data_from_url_op(\n",
    "        url = 'https://earthquake.usgs.gov/fdsnws/event/1/query',\n",
    "        delta_days = 3,\n",
    "    )\n",
    "\n",
    "    with dsl.Condition(raw_data.outputs['Output'] != '', name = 'Download ok'):\n",
    "        \n",
    "        save_data_to_gcp_op(\n",
    "            file_to_save = raw_data.outputs['downloaded_data'],\n",
    "            file_name = raw_data.outputs['Output'],\n",
    "            bucket_name = maint_bucket,\n",
    "            bucket_folder = raw_location,\n",
    "        )\n",
    "\n",
    "        cleaned_data = clean_data_op(\n",
    "            raw_data.outputs['downloaded_data'],\n",
    "        )\n",
    "\n",
    "        merged_data = merge_new_data_op(\n",
    "            new_data = cleaned_data.outputs['cleaned_data'],\n",
    "            bucket_name = maint_bucket,\n",
    "            file_path = silver_location,\n",
    "            file_name = 'silver.csv',\n",
    "        )\n",
    "\n",
    "        with dsl.Condition(merged_data.outputs['Output'] == 'True', name = 'New info'):\n",
    "            \n",
    "            save_df_to_gcp_op(\n",
    "                file_to_save = merged_data.outputs['silver_data'],\n",
    "                file_name = 'silver',\n",
    "                suffix = '',\n",
    "                bucket_name = maint_bucket,\n",
    "                bucket_folder = silver_location,\n",
    "            )\n",
    "            \n",
    "        problem_statement_config = get_problem_statement_op(\n",
    "            bucket_name = maint_bucket,\n",
    "            source_file = problem_statement_file,\n",
    "        )\n",
    "\n",
    "        filtered_data = filter_and_preprocess_op(\n",
    "            silver = merged_data.outputs['silver_data'],\n",
    "            event_reference = problem_statement_config.outputs['event_reference'],\n",
    "            min_latitude = problem_statement_config.outputs['min_latitude'],\n",
    "            max_latitude = problem_statement_config.outputs['max_latitude'],\n",
    "            min_longitude = problem_statement_config.outputs['min_longitude'],\n",
    "            max_longitude = problem_statement_config.outputs['max_longitude'],\n",
    "            time_cut = problem_statement_config.outputs['time_cut'],\n",
    "        )\n",
    "\n",
    "        grid_data = reindex_df_op(\n",
    "            filtered = filtered_data.outputs['filtered'],\n",
    "            degrees_latitude_grid = problem_statement_config.outputs['degrees_latitude_grid'],\n",
    "            km_depth_grid = problem_statement_config.outputs['km_depth_grid'],\n",
    "            pad_df = problem_statement_config.outputs['pad_df'],\n",
    "            main_id = problem_statement_config.outputs['main_id'],\n",
    "            time_ref = problem_statement_config.outputs['time_ref'],\n",
    "            time_frequency = problem_statement_config.outputs['time_frequency'],\n",
    "            date_offset = date_offset,\n",
    "            is_training = is_training,\n",
    "        )\n",
    "\n",
    "        full_data = get_neighbours_df_op(\n",
    "            grid = grid_data.outputs['grid'],\n",
    "            x_degre_km = grid_data.outputs['x_degre_km'],\n",
    "            y_degre_km = grid_data.outputs['y_degre_km'],\n",
    "            dx = grid_data.outputs['dx'],\n",
    "            dy = grid_data.outputs['dy'],\n",
    "            range_x_min = grid_data.outputs['range_x_min'],\n",
    "            range_x_max = grid_data.outputs['range_x_max'],\n",
    "            range_x_step = grid_data.outputs['range_x_step'],\n",
    "            range_y_min = grid_data.outputs['range_y_min'],\n",
    "            range_y_max = grid_data.outputs['range_y_max'],\n",
    "            range_y_step = grid_data.outputs['range_y_step'],\n",
    "            range_z_min = grid_data.outputs['range_z_min'],\n",
    "            range_z_max = grid_data.outputs['range_z_max'],\n",
    "            range_z_step = grid_data.outputs['range_z_step'],\n",
    "        )\n",
    "\n",
    "        ps_data = set_problem_statement_op(\n",
    "            full = full_data.outputs['full'],\n",
    "            main_id = problem_statement_config.outputs['main_id'],\n",
    "            time_ref = problem_statement_config.outputs['time_ref'],\n",
    "            target_raw = problem_statement_config.outputs['target_raw'],\n",
    "            early_warning_number = problem_statement_config.outputs['early_warning_number'],\n",
    "            range_warning_number = problem_statement_config.outputs['range_warning_number'],\n",
    "            is_training = is_training,\n",
    "        )\n",
    "\n",
    "        gold_data = feature_engineering_op(\n",
    "            ps = ps_data.outputs['ps'],\n",
    "            main_id = problem_statement_config.outputs['main_id'],\n",
    "            time_ref = problem_statement_config.outputs['time_ref'],\n",
    "            target_raw = problem_statement_config.outputs['target_raw'],\n",
    "        )\n",
    "\n",
    "        save_df_to_gcp_op(\n",
    "            file_to_save = gold_data.outputs['gold_data'],\n",
    "            file_name = 'gold',\n",
    "            suffix = str(date_offset),\n",
    "            bucket_name = maint_bucket,\n",
    "            bucket_folder = gold_location,\n",
    "        )\n",
    "        \n",
    "        predictions = make_predictions_op(\n",
    "            gold_data = gold_data.outputs['gold_data'],\n",
    "            bucket_name = maint_bucket,\n",
    "            artifact_folder = artifact_location,\n",
    "            step_x = grid_data.outputs['range_x_step'],\n",
    "            step_y = grid_data.outputs['range_y_step'],\n",
    "            step_z = grid_data.outputs['range_z_step'],\n",
    "            time_frequency = problem_statement_config.outputs['time_frequency'],\n",
    "            early_warning_number = problem_statement_config.outputs['early_warning_number'],\n",
    "            range_warning_number = problem_statement_config.outputs['range_warning_number'],\n",
    "            date_offset = date_offset,\n",
    "        )\n",
    "        \n",
    "        with dsl.Condition(predictions.outputs['Output'] != '', name = 'New prediction'):\n",
    "\n",
    "            df_saved = save_df_to_gcp_op(\n",
    "                file_to_save = predictions.outputs['predictions_data'],\n",
    "                file_name = 'predictions',\n",
    "                suffix = predictions.outputs['Output'],\n",
    "                bucket_name = maint_bucket,\n",
    "                bucket_folder = predictions_location,\n",
    "            )\n",
    "            \n",
    "            calculate_metrics_op(\n",
    "                dummy_input = df_saved.outputs['Output'],\n",
    "                main_bucket_name = maint_bucket,\n",
    "                silver_location = silver_location,\n",
    "                predictions_location = predictions_location,\n",
    "                metrics_location = metrics_location,\n",
    "                event_reference = problem_statement_config.outputs['event_reference'],\n",
    "                min_latitude = problem_statement_config.outputs['min_latitude'],\n",
    "                max_latitude = problem_statement_config.outputs['max_latitude'],\n",
    "                min_longitude = problem_statement_config.outputs['min_longitude'],\n",
    "                max_longitude = problem_statement_config.outputs['max_longitude'],\n",
    "            )\n",
    "\n",
    "    return\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func = pipeline,\n",
    "    package_path = pipeline_name.replace('-', '_') + '.json'\n",
    ")\n",
    "\n",
    "save_pipeline(pipeline_name.replace('-', '_') + '.json', main_bucket_name, 'pipelines/json/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import datetime\n",
    "\n",
    "# api_client = AIPlatformClient(\n",
    "#     project_id = project_id,\n",
    "#     region = region\n",
    "# )\n",
    "# parameter_values = {\n",
    "#     'is_training': 'False',\n",
    "#     'maint_bucket': main_bucket_name,\n",
    "#     'problem_statement_file': problem_statement_file,\n",
    "#     'date_offset': 5,\n",
    "#     'raw_location': raw_location,\n",
    "#     'silver_location': silver_location,\n",
    "#     'gold_location': gold_location,\n",
    "#     'artifact_location': artifact_location,\n",
    "#     'predictions_location': predictions_location,\n",
    "#     'metrics_location': metrics_location,\n",
    "# }\n",
    "\n",
    "# run_time = datetime.now().strftime('%Y%m%d%H%m%S%f')\n",
    "\n",
    "# api_client.create_run_from_job_spec(\n",
    "#     job_spec_path = pipeline_root_path + 'json/' + pipeline_name.replace('-', '_') + '.json',\n",
    "#     job_id = pipeline_name.replace('-', '') + '{0}'.format(run_time),\n",
    "#     pipeline_root = pipeline_root_path,\n",
    "#     enable_caching = False,\n",
    "#     parameter_values = parameter_values\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def local_get_data_from_url(url, delta_days, end_date):\n",
    "\n",
    "#     import requests\n",
    "#     from datetime import datetime, timedelta\n",
    "#     import pandas as pd\n",
    "\n",
    "#     # get resquest params\n",
    "#     if end_date is None:\n",
    "#         end_range = datetime.now() + timedelta(days = 1)\n",
    "#     else:\n",
    "#         end_range = end_date - timedelta(days = 1)\n",
    "#     begin_range = end_range - timedelta(days = delta_days)\n",
    "#     end_range = end_range.strftime('%Y-%m-%d')\n",
    "#     begin_range = begin_range.strftime('%Y-%m-%d')\n",
    "\n",
    "#     query = {'format': 'csv', 'starttime': begin_range, 'endtime': end_range}\n",
    "    \n",
    "#     # get time\n",
    "#     request_name = begin_range + '_' + end_range + '_.csv'\n",
    "\n",
    "#     response_content = None\n",
    "\n",
    "#     # make request\n",
    "#     try:\n",
    "#         with requests.get(url, params = query) as response:\n",
    "#             response.raise_for_status()\n",
    "#             response_content = response.content\n",
    "#     except requests.exceptions.Timeout:\n",
    "#         print('Timeout Exception')\n",
    "#         return ''\n",
    "#     except requests.exceptions.TooManyRedirects:\n",
    "#         print('Too Many Redirects Exception')\n",
    "#         return ''\n",
    "#     except requests.exceptions.HTTPError:\n",
    "#         print(query)\n",
    "#         print('Http Exception')\n",
    "#         return ''\n",
    "#     except requests.exceptions.ConnectionError:\n",
    "#         print('Error Connecting')\n",
    "#         return ''\n",
    "#     except requests.exceptions.RequestException:\n",
    "#         print('Request Exception')\n",
    "#         return ''\n",
    "#     except:\n",
    "#         print('Error requesting file')\n",
    "#         return ''\n",
    "\n",
    "#     # dump for next component\n",
    "#     with open('./temp/' + request_name, 'w') as text_file:\n",
    "#         text_file.write(response_content.decode('utf-8'))\n",
    "        \n",
    "#     df = pd.read_csv('./temp/' + request_name)\n",
    "    \n",
    "#     df['date'] = pd.to_datetime(df['time'].str.split('T').str[0], format = '%Y-%m-%d', exact = True)\n",
    "    \n",
    "#     assert((df['date'].max() - df['date'].min()).days + 1 == delta_days == df['date'].nunique())\n",
    "\n",
    "#     return df['date'].min()\n",
    "\n",
    "# min_date = None\n",
    "\n",
    "# for i in range(25):\n",
    "#     min_date = local_get_data_from_url('https://earthquake.usgs.gov/fdsnws/event/1/query', 29, min_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from os import listdir\n",
    "# from os.path import isfile, join\n",
    "# import pandas as pd\n",
    "\n",
    "# onlyfiles = [f for f in listdir('./temp/') if f.endswith('.csv')]\n",
    "\n",
    "# dfs = []\n",
    "\n",
    "# for f in onlyfiles:\n",
    "#     dfs.append(pd.read_csv('./temp/' + f))\n",
    "    \n",
    "# df = pd.concat(dfs)\n",
    "\n",
    "# assert(df['id'].nunique() == len(df))\n",
    "\n",
    "# def local_clean_data(df):\n",
    "\n",
    "#     import joblib\n",
    "#     import pandas as pd\n",
    "    \n",
    "#     # select info to be used\n",
    "#     df = df.loc[:, ['time', 'id', 'latitude', 'longitude', 'depth', 'mag']].copy()\n",
    "    \n",
    "#     # weird values\n",
    "#     z_0 = df['depth'] < 0\n",
    "#     print(f'Depth above sup: {sum(z_0):,d} ({sum(z_0) / len(df):.2%})')\n",
    "#     df.loc[z_0, 'depth'] = 0\n",
    "    \n",
    "#     # data parsing\n",
    "#     date_col = 'time'\n",
    "#     datetimes = df[date_col].str.split('T', expand = True)\n",
    "#     dates = pd.to_datetime(datetimes.loc[:, 0], format = '%Y-%m-%d')\n",
    "#     df = pd.concat((df, dates.rename('date')), axis = 1)\n",
    "#     df = df.drop(date_col, axis = 1)\n",
    "    \n",
    "#     # drop NA\n",
    "#     len_before = len(df)\n",
    "#     df = df.dropna()\n",
    "#     len_after = len(df)\n",
    "#     dropped_events = len_before - len_after\n",
    "#     if (dropped_events) == 0:\n",
    "#         print('No dropped events')\n",
    "#     else:\n",
    "#         print(f'Dropped events: {dropped_events:,d} ({dropped_events / len_before:.2%})')\n",
    "        \n",
    "#     return df\n",
    "\n",
    "# df = local_clean_data(df)\n",
    "\n",
    "# df.to_csv('silver.csv', index = False)\n",
    "\n",
    "# df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m79",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m79"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
